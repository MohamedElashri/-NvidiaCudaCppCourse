{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the 3 sections in **bold** above. In the next lab, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `CUDA Kernel Statistics` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-40e7-2610-3867-2ea3.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-40e7-2610-3867-2ea3.qdrep\"\n",
      "Exporting 4687 events: [==================================================100%]                                                    ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-40e7-2610-3867-2ea3.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average      Minimum     Maximum            Name         \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  ---------------------\n",
      "    83.6       2314130543          1  2314130543.0  2314130543  2314130543  cudaDeviceSynchronize\n",
      "    15.6        432055706          3   144018568.7       18523   431979186  cudaMallocManaged    \n",
      "     0.8         20942604          3     6980868.0     6321689     8131064  cudaFree             \n",
      "     0.0            42863          1       42863.0       42863       42863  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average      Minimum     Maximum                       Name                    \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  -------------------------------------------\n",
      "   100.0       2314117887          1  2314117887.0  2314117887  2314117887  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.6         68416173        2304  29694.5     1886   171419  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.4         20951281         768  27280.3     1151   159708  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    88.7       5539516946        275  20143698.0    44877  100135513  poll                      \n",
      "     8.1        508815056        242   2102541.6    24012   20645749  sem_timedwait             \n",
      "     2.7        167175298        687    243341.0     1021   31021374  ioctl                     \n",
      "     0.4         26310014         98    268469.5     1582    8058764  mmap                      \n",
      "     0.0          2655678         82     32386.3     4711      67302  open64                    \n",
      "     0.0           214092          3     71364.0    69365      75190  fgets                     \n",
      "     0.0           171080          4     42770.0    33173      52508  pthread_create            \n",
      "     0.0           163577         25      6543.1     1643      32172  fopen                     \n",
      "     0.0           131066         11     11915.1     6215      23941  write                     \n",
      "     0.0           103840         65      1597.5     1001       6890  fcntl                     \n",
      "     0.0            63944         11      5813.1     1776      10893  munmap                    \n",
      "     0.0            54587          7      7798.1     1170      25790  fgetc                     \n",
      "     0.0            35309          5      7061.8     3150       9241  open                      \n",
      "     0.0            33254         18      1847.4     1112       7152  fclose                    \n",
      "     0.0            28723          3      9574.3     3029      16011  fread                     \n",
      "     0.0            25799         13      1984.5     1032       2916  read                      \n",
      "     0.0            20944          2     10472.0     9601      11343  socket                    \n",
      "     0.0            19674          1     19674.0    19674      19674  sem_wait                  \n",
      "     0.0            15245          1     15245.0    15245      15245  pipe2                     \n",
      "     0.0            10009          4      2502.3     1750       3359  mprotect                  \n",
      "     0.0             8916          1      8916.0     8916       8916  connect                   \n",
      "     0.0             3720          1      3720.0     3720       3720  bind                      \n",
      "     0.0             2162          1      2162.0     2162       2162  listen                    \n",
      "     0.0             1884          1      1884.0     1884       1884  pthread_rwlock_timedwrlock\n",
      "\n",
      "Report file moved to \"/dli/task/report1.qdrep\"\n",
      "Report file moved to \"/dli/task/report1.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-6425-c2a0-cc13-e9e0.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6425-c2a0-cc13-e9e0.qdrep\"\n",
      "Exporting 4379 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6425-c2a0-cc13-e9e0.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    74.5        775159338          1  775159338.0  775159338  775159338  cudaDeviceSynchronize\n",
      "    23.5        243933212          3   81311070.7      25414  243828090  cudaMallocManaged    \n",
      "     2.0         20854232          3    6951410.7    6318828    8105410  cudaFree             \n",
      "     0.0            69752          1      69752.0      69752      69752  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        775153222          1  775153222.0  775153222  775153222  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.6         68385673        2304  29681.3     1855   171358  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.4         20948180         768  27276.3     1150   160478  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    87.5       2609342534        136  19186342.2    36820  100551723  poll                      \n",
      "     8.6        256027159        121   2115926.9    10991   20580848  sem_timedwait             \n",
      "     3.1         92097828        686    134253.4     1016   17052233  ioctl                     \n",
      "     0.8         23365445         98    238422.9     1404    8027969  mmap                      \n",
      "     0.1          1551146         82     18916.4     9278      33440  open64                    \n",
      "     0.0           229999          3     76666.3    70630      83470  fgets                     \n",
      "     0.0           180471          4     45117.8    32932      62826  pthread_create            \n",
      "     0.0           152322         25      6092.9     1543      28583  fopen                     \n",
      "     0.0            94942         11      8631.1     4142      12634  write                     \n",
      "     0.0            53902         11      4900.2     2059       8859  munmap                    \n",
      "     0.0            37507          5      7501.4     5041      10343  open                      \n",
      "     0.0            33879         18      1882.2     1155       4838  fclose                    \n",
      "     0.0            28488         13      2191.4     1026       3204  read                      \n",
      "     0.0            27128         19      1427.8     1000       6555  fcntl                     \n",
      "     0.0            24040          3      8013.3     6371       9834  pthread_rwlock_timedwrlock\n",
      "     0.0            22602          5      4520.4     1056      11314  fgetc                     \n",
      "     0.0            16594          2      8297.0     6414      10180  socket                    \n",
      "     0.0             9879          1      9879.0     9879       9879  connect                   \n",
      "     0.0             9452          3      3150.7     1480       4112  fread                     \n",
      "     0.0             8127          1      8127.0     8127       8127  pipe2                     \n",
      "     0.0             7961          4      1990.3     1798       2286  mprotect                  \n",
      "     0.0             2414          1      2414.0     2414       2414  bind                      \n",
      "     0.0             1755          1      1755.0     1755       1755  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report2.qdrep\"\n",
      "Report file moved to \"/dli/task/report2.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-0a6e-5326-de37-89ce.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-0a6e-5326-de37-89ce.qdrep\"\n",
      "Exporting 4426 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-0a6e-5326-de37-89ce.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average      Minimum     Maximum            Name         \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  ---------------------\n",
      "    80.9       1067396196          1  1067396196.0  1067396196  1067396196  cudaDeviceSynchronize\n",
      "    17.5        230434297          3    76811432.3       16797   230369926  cudaMallocManaged    \n",
      "     1.6         21012225          3     7004075.0     6325702     8162044  cudaFree             \n",
      "     0.0            45541          1       45541.0       45541       45541  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average      Minimum     Maximum                       Name                    \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  -------------------------------------------\n",
      "   100.0       1067383016          1  1067383016.0  1067383016  1067383016  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.6         68427907        2304  29699.6     1886   171391  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.4         20949121         768  27277.5     1119   159775  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    88.3       3171417857        165  19220714.3    31395  100133335  poll                      \n",
      "     8.6        309418475        147   2104887.6    13627   20895477  sem_timedwait             \n",
      "     2.4         85234459        679    125529.4     1020   17211873  ioctl                     \n",
      "     0.7         23375117         98    238521.6     1269    8079778  mmap                      \n",
      "     0.0          1554873         82     18961.9     4645      33917  open64                    \n",
      "     0.0           214981          3     71660.3    69702      75562  fgets                     \n",
      "     0.0           184009          4     46002.3    34708      53358  pthread_create            \n",
      "     0.0           155836         25      6233.4     1550      27983  fopen                     \n",
      "     0.0            95114         11      8646.7     4689      17172  write                     \n",
      "     0.0            56965         11      5178.6     1525       9342  munmap                    \n",
      "     0.0            35380          5      7076.0     3085      10082  open                      \n",
      "     0.0            31604         18      1755.8     1152       5234  fclose                    \n",
      "     0.0            27277          6      4546.2     1025      12293  fgetc                     \n",
      "     0.0            26152         19      1376.4     1003       4791  fcntl                     \n",
      "     0.0            24281         13      1867.8     1077       3335  read                      \n",
      "     0.0            18445          2      9222.5     7759      10686  socket                    \n",
      "     0.0            10588          3      3529.3     1744       4706  fread                     \n",
      "     0.0             9880          1      9880.0     9880       9880  connect                   \n",
      "     0.0             9305          1      9305.0     9305       9305  pipe2                     \n",
      "     0.0             8411          4      2102.8     1823       2477  mprotect                  \n",
      "     0.0             3292          1      3292.0     3292       3292  bind                      \n",
      "     0.0             1711          1      1711.0     1711       1711  listen                    \n",
      "     0.0             1193          1      1193.0     1193       1193  pthread_rwlock_timedwrlock\n",
      "\n",
      "Report file moved to \"/dli/task/report3.qdrep\"\n",
      "Report file moved to \"/dli/task/report3.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 40\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 5\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-2fc5-960c-66ee-86e1.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-2fc5-960c-66ee-86e1.qdrep\"\n",
      "Exporting 1433 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-2fc5-960c-66ee-86e1.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    53.7        249216616          3  83072205.3     17988  249148058  cudaMallocManaged    \n",
      "    27.8        129188671          7  18455524.4      5889   32828965  cudaMemPrefetchAsync \n",
      "    12.9         59761293          1  59761293.0  59761293   59761293  cudaDeviceSynchronize\n",
      "     5.6         26239190          3   8746396.7   6302015   12459495  cudaFree             \n",
      "     0.0            36229          1     36229.0     36229      36229  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1707233          1  1707233.0  1707233  1707233  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    76.3         65681894         192  342093.2   339802   345754  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.7         20442160          64  319408.8   319098   322298  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    72.1       1164401071         68  17123545.2    26461  100129596  poll                      \n",
      "    16.7        269059685        684    393362.1     1054   32731663  ioctl                     \n",
      "     8.1        131132414         56   2341650.3    10595   20841453  sem_timedwait             \n",
      "     1.8         28580072         99    288687.6     1475   12370495  mmap                      \n",
      "     1.2         19821634          3   6607211.3    46070   12371495  sem_wait                  \n",
      "     0.1          1783676         82     21752.1     6558      35542  open64                    \n",
      "     0.0           238080          5     47616.0    31918      66234  pthread_create            \n",
      "     0.0           215862          3     71954.0    69574      76152  fgets                     \n",
      "     0.0           136649         25      5466.0     1653      23560  fopen                     \n",
      "     0.0           106182         13      8167.8     4062      13351  write                     \n",
      "     0.0            54010         11      4910.0     1739       8158  munmap                    \n",
      "     0.0            34011          5      6802.2     3372       9832  open                      \n",
      "     0.0            28824         18      1601.3     1037       4704  fclose                    \n",
      "     0.0            26488         15      1765.9     1056       2891  read                      \n",
      "     0.0            25859          6      4309.8     1042      10948  fgetc                     \n",
      "     0.0            19431          2      9715.5     7574      11857  socket                    \n",
      "     0.0            13220          1     13220.0    13220      13220  pthread_rwlock_timedwrlock\n",
      "     0.0            12345          8      1543.1     1039       4417  fcntl                     \n",
      "     0.0            11357          5      2271.4     1774       3559  mprotect                  \n",
      "     0.0            10147          3      3382.3     1688       4761  fread                     \n",
      "     0.0             9260          1      9260.0     9260       9260  connect                   \n",
      "     0.0             8193          1      8193.0     8193       8193  pipe2                     \n",
      "     0.0             2920          1      2920.0     2920       2920  bind                      \n",
      "     0.0             1806          1      1806.0     1806       1806  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report4.qdrep\"\n",
      "Report file moved to \"/dli/task/report4.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-d710-6201-66d2-e8c3.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-d710-6201-66d2-e8c3.qdrep\"\n",
      "Exporting 1892 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-d710-6201-66d2-e8c3.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    89.5        290736118          1  290736118.0  290736118  290736118  cudaMallocManaged    \n",
      "     6.3         20556688          1   20556688.0   20556688   20556688  cudaDeviceSynchronize\n",
      "     4.2         13623092          1   13623092.0   13623092   13623092  cudaFree             \n",
      "     0.0            82480          1      82480.0      82480      82480  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum            Name          \n",
      " -------  ---------------  ---------  ----------  --------  --------  -----------------------\n",
      "   100.0         20544389          1  20544389.0  20544389  20544389  deviceKernel(int*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21450585         768  27930.4     1631   168990  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    79.1        764161867         42  18194330.2    27232  100155593  poll                      \n",
      "    10.2         98172732        666    147406.5     1052   17660033  ioctl                     \n",
      "     8.7         84203770         37   2275777.6     8775   20728909  sem_timedwait             \n",
      "     1.7         16878993         92    183467.3     1611   13408338  mmap                      \n",
      "     0.2          2076668         82     25325.2     5036      48470  open64                    \n",
      "     0.0           216409          3     72136.3    69459      76773  fgets                     \n",
      "     0.0           201029          4     50257.3    35905      61492  pthread_create            \n",
      "     0.0           178603         25      7144.1     1532      42414  fopen                     \n",
      "     0.0            99910         11      9082.7     4706      17717  write                     \n",
      "     0.0            70882         47      1508.1     1010      10166  fcntl                     \n",
      "     0.0            49301          7      7043.0     3262      14657  munmap                    \n",
      "     0.0            38190         18      2121.7     1161      10322  fclose                    \n",
      "     0.0            35985          5      7197.0     3117      10094  open                      \n",
      "     0.0            33294          4      8323.5     1118      28775  pthread_rwlock_timedwrlock\n",
      "     0.0            27705          2     13852.5    10252      17453  fread                     \n",
      "     0.0            27032          6      4505.3     1101      12463  fgetc                     \n",
      "     0.0            22076         12      1839.7     1251       2920  read                      \n",
      "     0.0            19461          2      9730.5     9558       9903  socket                    \n",
      "     0.0            15871          1     15871.0    15871      15871  sem_wait                  \n",
      "     0.0             8764          4      2191.0     2027       2588  mprotect                  \n",
      "     0.0             8563          1      8563.0     8563       8563  pipe2                     \n",
      "     0.0             8200          1      8200.0     8200       8200  connect                   \n",
      "     0.0             3388          1      3388.0     3388       3388  bind                      \n",
      "     0.0             1872          1      1872.0     1872       1872  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report5.qdrep\"\n",
      "Report file moved to \"/dli/task/report5.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-600c-628e-c15e-023a.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-600c-628e-c15e-023a.qdrep\"\n",
      "Exporting 1449 events: [==================================================100%]                         ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-600c-628e-c15e-023a.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    53.7        259652313          3  86550771.0     19233  259550960  cudaMallocManaged    \n",
      "    33.4        161637493          7  23091070.4    181099   39525989  cudaMemPrefetchAsync \n",
      "     7.6         36786947          1  36786947.0  36786947   36786947  cudaDeviceSynchronize\n",
      "     5.3         25550438          3   8516812.7   6283509   10110083  cudaFree             \n",
      "     0.0            35403          1     35403.0     35403      35403  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1698121          1  1698121.0  1698121  1698121  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    76.2         65480648         192  341045.0   339803   346203  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.8         20470156          64  319846.2   319132   325275  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    71.8       1155587823         65  17778274.2    31900  100134506  poll                      \n",
      "    17.8        286262421        686    417292.2     1061   39449817  ioctl                     \n",
      "     7.7        124415633         55   2262102.4    11041   20911813  sem_timedwait             \n",
      "     1.7         28106372         99    283902.7     1472    9899770  mmap                      \n",
      "     0.8         12334312          3   4111437.3    13420   12296720  sem_wait                  \n",
      "     0.1          2135805         82     26046.4     4575      42470  open64                    \n",
      "     0.0           275625          5     55125.0    34931      83320  pthread_create            \n",
      "     0.0           215414          3     71804.7    69738      75537  fgets                     \n",
      "     0.0           146587         25      5863.5     1556      28019  fopen                     \n",
      "     0.0           111052         12      9254.3     4171      15896  write                     \n",
      "     0.0            96707         12      8058.9     1514      38251  munmap                    \n",
      "     0.0            36946          5      7389.2     4443       9722  open                      \n",
      "     0.0            33942         26      1305.5     1003       4620  fcntl                     \n",
      "     0.0            30452         18      1691.8     1040       5246  fclose                    \n",
      "     0.0            29778         14      2127.0     1009       3456  read                      \n",
      "     0.0            28442          6      4740.3     1109      13589  fgetc                     \n",
      "     0.0            20422          2     10211.0     1065      19357  pthread_rwlock_timedwrlock\n",
      "     0.0            20155          2     10077.5     9493      10662  socket                    \n",
      "     0.0            13463          5      2692.6     2116       3484  mprotect                  \n",
      "     0.0            11407          3      3802.3     1800       5225  fread                     \n",
      "     0.0             9322          1      9322.0     9322       9322  connect                   \n",
      "     0.0             8740          1      8740.0     8740       8740  pipe2                     \n",
      "     0.0             2864          1      2864.0     2864       2864  bind                      \n",
      "     0.0             1750          1      1750.0     1750       1750  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report6.qdrep\"\n",
      "Report file moved to \"/dli/task/report6.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-3477-8727-1ca8-f7f4.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-3477-8727-1ca8-f7f4.qdrep\"\n",
      "Exporting 1503 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-3477-8727-1ca8-f7f4.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  --------  ---------  ---------------------\n",
      "    59.8        408032816          3  136010938.7     37834  407836856  cudaMallocManaged    \n",
      "    28.4        193655189          7   27665027.0      7428   63994516  cudaMemPrefetchAsync \n",
      "     8.7         59488536          1   59488536.0  59488536   59488536  cudaDeviceSynchronize\n",
      "     3.1         20893901          3    6964633.7   6230116    8113035  cudaFree             \n",
      "     0.0            41364          1      41364.0     41364      41364  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1704329          1  1704329.0  1704329  1704329  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    76.2         65693981         192  342156.2   339835   346844  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.8         20471824          64  319872.3   319132   324956  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    72.0       1383475129         70  19763930.4    32791  100140489  poll                      \n",
      "    19.0        365641525        692    528383.7     1038   63775949  ioctl                     \n",
      "     6.8        129975767         56   2320995.8    16927   20830987  sem_timedwait             \n",
      "     1.4         25959674         99    262218.9     1961    8024093  mmap                      \n",
      "     0.7         12544526          2   6272263.0    46964   12497562  sem_wait                  \n",
      "     0.2          3018849         82     36815.2     8396      65628  open64                    \n",
      "     0.0           289223          5     57844.6    50224      63722  pthread_create            \n",
      "     0.0           278157          3     92719.0    88481      95303  fgets                     \n",
      "     0.0           204383         12     17031.9     8727      42843  write                     \n",
      "     0.0           198267         25      7930.7     2460      38170  fopen                     \n",
      "     0.0           131020         11     11910.9     3575      53554  munmap                    \n",
      "     0.0           120062         66      1819.1     1014       9732  fcntl                     \n",
      "     0.0            59232          6      9872.0     1157      19289  pthread_rwlock_timedwrlock\n",
      "     0.0            46778          5      9355.6     6197      12411  open                      \n",
      "     0.0            46711         18      2595.1     1374       9972  fclose                    \n",
      "     0.0            44525          7      6360.7     1557      13846  fgetc                     \n",
      "     0.0            26872         13      2067.1     1022       4122  read                      \n",
      "     0.0            20484          2     10242.0    10209      10275  socket                    \n",
      "     0.0            19549          1     19549.0    19549      19549  pipe2                     \n",
      "     0.0            18602          3      6200.7     3068       8479  fread                     \n",
      "     0.0            12841          5      2568.2     1813       3458  mprotect                  \n",
      "     0.0            11086          1     11086.0    11086      11086  connect                   \n",
      "     0.0             3896          1      3896.0     3896       3896  bind                      \n",
      "     0.0             2495          1      2495.0     2495       2495  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report7.qdrep\"\n",
      "Report file moved to \"/dli/task/report7.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-29ba-3447-34a7-5c60.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-29ba-3447-34a7-5c60.qdrep\"\n",
      "Exporting 1479 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-29ba-3447-34a7-5c60.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  --------  ---------  ---------------------\n",
      "    51.3        311526130          3  103842043.3     45859  311372212  cudaMallocManaged    \n",
      "    35.5        215740597          7   30820085.3      6409   62024998  cudaMemPrefetchAsync \n",
      "     9.8         59659017          1   59659017.0  59659017   59659017  cudaDeviceSynchronize\n",
      "     3.4         20377987          3    6792662.3   6198456    7790897  cudaFree             \n",
      "     0.0            33900          1      33900.0     33900      33900  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1699785          1  1699785.0  1699785  1699785  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    76.2         65681015         192  342088.6   339867   348059  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.8         20458643          64  319666.3   319164   324187  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    72.6       1415852206         74  19133137.9    37509  100140841  poll                      \n",
      "    18.0        351077918        692    507338.0     1066   61838149  ioctl                     \n",
      "     7.3        142108601         60   2368476.7    20330   20736160  sem_timedwait             \n",
      "     1.2         23916810         99    241583.9     1509    7703826  mmap                      \n",
      "     0.6         12542968          3   4180989.3    25343   12456640  sem_wait                  \n",
      "     0.1          2177262         82     26552.0     6575      47313  open64                    \n",
      "     0.0           288641          3     96213.7    93113     101014  fgets                     \n",
      "     0.0           243460          5     48692.0    37455      69781  pthread_create            \n",
      "     0.0           196573         25      7862.9     2036      41491  fopen                     \n",
      "     0.0           142793         12     11899.4     5513      39233  write                     \n",
      "     0.0            72020         11      6547.3     3182      13881  munmap                    \n",
      "     0.0            50981         35      1456.6     1000       9193  fcntl                     \n",
      "     0.0            49671          4     12417.8     1119      33176  pthread_rwlock_timedwrlock\n",
      "     0.0            42970          5      8594.0     4661      12742  open                      \n",
      "     0.0            42264         18      2348.0     1333       9571  fclose                    \n",
      "     0.0            32381         14      2312.9     1301       3926  read                      \n",
      "     0.0            27066          6      4511.0     1220      11482  fgetc                     \n",
      "     0.0            19853          2      9926.5     8300      11553  socket                    \n",
      "     0.0            17439          3      5813.0     3024       7262  fread                     \n",
      "     0.0            12606          5      2521.2     1956       3741  mprotect                  \n",
      "     0.0            12596          1     12596.0    12596      12596  connect                   \n",
      "     0.0             9600          1      9600.0     9600       9600  pipe2                     \n",
      "     0.0             3125          1      3125.0     3125       3125  bind                      \n",
      "     0.0             1994          1      1994.0     1994       1994  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report8.qdrep\"\n",
      "Report file moved to \"/dli/task/report8.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-3d21-2aaa-15af-7a14.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-3d21-2aaa-15af-7a14.qdrep\"\n",
      "Exporting 1457 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-3d21-2aaa-15af-7a14.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    53.4        248341695          3  82780565.0     17846  248281297  cudaMallocManaged    \n",
      "    29.3        136356652          7  19479521.7      8713   39484104  cudaMemPrefetchAsync \n",
      "    12.9         59832513          1  59832513.0  59832513   59832513  cudaDeviceSynchronize\n",
      "     4.4         20556465          3   6852155.0   6240888    7914557  cudaFree             \n",
      "     0.0            42219          1     42219.0     42219      42219  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1693706          1  1693706.0  1693706  1693706  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    76.2         65675409         192  342059.4   339836   345340  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.8         20491957          64  320186.8   319163   328123  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    72.6       1145297910         65  17619967.8    49695  100148117  poll                      \n",
      "    17.3        273475926        692    395196.4     1000   39409501  ioctl                     \n",
      "     7.6        119070648         55   2164920.9    15279   20895976  sem_timedwait             \n",
      "     1.5         23254308         99    234892.0     1557    7795725  mmap                      \n",
      "     0.8         12508742          2   6254371.0    33164   12475578  sem_wait                  \n",
      "     0.1          1938559         82     23641.0     6222      55518  open64                    \n",
      "     0.0           246115          5     49223.0    32815      69706  pthread_create            \n",
      "     0.0           216269          3     72089.7    69903      76275  fgets                     \n",
      "     0.0           154820         12     12901.7     4509      31932  write                     \n",
      "     0.0           140268         25      5610.7     1586      25542  fopen                     \n",
      "     0.0            58989         11      5362.6     1557       9159  munmap                    \n",
      "     0.0            46676         29      1609.5     1049       6001  fcntl                     \n",
      "     0.0            38758          3     12919.3     9979      14461  pthread_rwlock_timedwrlock\n",
      "     0.0            34919          5      6983.8     3157       9853  open                      \n",
      "     0.0            31350         18      1741.7     1077       5738  fclose                    \n",
      "     0.0            29407         14      2100.5     1240       3787  read                      \n",
      "     0.0            27321          6      4553.5     1060      13164  fgetc                     \n",
      "     0.0            24930          3      8310.0     2551      16463  fread                     \n",
      "     0.0            19484          2      9742.0     7821      11663  socket                    \n",
      "     0.0            11200          5      2240.0     1724       3346  mprotect                  \n",
      "     0.0             9357          1      9357.0     9357       9357  pipe2                     \n",
      "     0.0             8743          1      8743.0     8743       8743  connect                   \n",
      "     0.0             2920          1      2920.0     2920       2920  bind                      \n",
      "     0.0             1879          1      1879.0     1879       1879  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report9.qdrep\"\n",
      "Report file moved to \"/dli/task/report9.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](../edit/09-saxpy/01-saxpy.cu). It currently contains a couple of bugs that you will need to find and fix before you can successfully compile, run, and then profile it with `nsys profile`.\n",
    "\n",
    "After fixing the bugs and profiling the application, record the runtime of the `saxpy` kernel and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200us*. Check out [the solution](../edit/09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \r\n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-b988-db5a-2dba-5100.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-b988-db5a-2dba-5100.qdrep\"\n",
      "Exporting 1156 events: [==================================================100%]]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-b988-db5a-2dba-5100.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    95.3        258190562          3  86063520.7    31405  258086973  cudaMallocManaged    \n",
      "     3.1          8301880          1   8301880.0  8301880    8301880  cudaDeviceSynchronize\n",
      "     1.0          2704836          3    901612.0   874918     938514  cudaFree             \n",
      "     0.6          1656745          3    552248.3     8944    1493968  cudaMemPrefetchAsync \n",
      "     0.0            41934          1     41934.0    41934      41934  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum           Name          \n",
      " -------  ---------------  ---------  --------  -------  -------  -----------------------\n",
      "   100.0           197822          1  197822.0   197822   197822  saxpy(int*, int*, int*)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    99.7          8217239          24  342385.0   339932   344731  [CUDA Unified Memory memcpy HtoD]\n",
      "     0.3            24638           4    6159.5     1695    10592  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total    Operations  Average   Minimum   Maximum               Operation            \n",
      " ---------  ----------  --------  --------  --------  ---------------------------------\n",
      " 49152.000          24  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      "   128.000           4    32.000     4.000    60.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    69.5        370625150         25  14825006.0    31995  100151867  poll                      \n",
      "    19.5        103802611        680    152650.9     1102   17835961  ioctl                     \n",
      "     8.8         46788289         17   2752252.3    11813   20604347  sem_timedwait             \n",
      "     1.1          5734634         98     58516.7     1666     972706  mmap                      \n",
      "     0.5          2848261          4    712065.3    15749    1653465  sem_wait                  \n",
      "     0.4          2113379         82     25772.9     6000      45539  open64                    \n",
      "     0.1           304347          5     60869.4    44887      75800  pthread_create            \n",
      "     0.0           216057          3     72019.0    69511      76071  fgets                     \n",
      "     0.0           195975         13     15075.0     4682      47147  write                     \n",
      "     0.0           155133         25      6205.3     1572      26954  fopen                     \n",
      "     0.0            69176         52      1330.3     1001       6691  fcntl                     \n",
      "     0.0            51490         14      3677.9     1369      20073  read                      \n",
      "     0.0            44647          9      4960.8     2438       8779  munmap                    \n",
      "     0.0            38206          5      7641.2     3812       9970  open                      \n",
      "     0.0            32746         18      1819.2     1153       5383  fclose                    \n",
      "     0.0            32075          6      5345.8     1207      14113  fgetc                     \n",
      "     0.0            22183          2     11091.5     1046      21137  pthread_rwlock_timedwrlock\n",
      "     0.0            19663          2      9831.5     8583      11080  socket                    \n",
      "     0.0            16719          2      8359.5     5186      11533  fread                     \n",
      "     0.0            14239          5      2847.8     1900       4246  mprotect                  \n",
      "     0.0            11493          1     11493.0    11493      11493  pipe2                     \n",
      "     0.0            10133          1     10133.0    10133      10133  connect                   \n",
      "     0.0             3277          1      3277.0     3277       3277  bind                      \n",
      "     0.0             1836          1      1836.0     1836       1836  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report10.qdrep\"\n",
      "Report file moved to \"/dli/task/report10.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
